Below is **2.3 SQL test endpoint code** (Fastify + pg) that supports your SQL Builder:

✅ runs SQL against a **target DB** (via `targetDatabaseUrl`)
✅ **safe limits**: single statement, timeout, row cap, auto LIMIT
✅ returns **sample rows + column metadata**
✅ returns **mapping suggestions** (`table|column|relationship`) based on result columns
✅ (optional) returns `sqlFinal` so UI can show what actually ran

---

# 1) Mapping suggestion heuristics

### `apps/api/src/sqlrunner/suggestMapping.ts`

```ts
import type { ResultMapping, TargetKind } from './mapping';

type Col = { name: string; dataTypeId?: number };

function lc(s: string) {
  return s.trim().toLowerCase();
}

function has(cols: string[], ...cands: string[]) {
  const set = new Set(cols);
  return cands.every((c) => set.has(c));
}

function pickExisting(cols: string[], options: string[]) {
  const set = new Set(cols);
  for (const o of options) if (set.has(o)) return o;
  return null;
}

export type MappingSuggestion = {
  targetKind: TargetKind | null;
  mapping: ResultMapping | null;
  confidence: 'high' | 'medium' | 'low' | 'none';
  reason: string;
};

export function suggestMapping(columns: Col[]): MappingSuggestion {
  const cols = columns.map((c) => lc(c.name));
  const set = new Set(cols);
  const list = Array.from(set);

  // Common field aliases
  const schemaCol = pickExisting(list, ['schema', 'schema_name', 'nspname', 'table_schema']);
  const tableCol = pickExisting(list, ['table', 'table_name', 'relname', 'table_name']);
  const columnCol = pickExisting(list, ['column', 'column_name', 'attname']);

  // Relationship candidates
  const childSchema = pickExisting(list, ['child_schema', 'childschema', 'src_schema', 'from_schema']);
  const childTable  = pickExisting(list, ['child_table', 'childtable', 'src_table', 'from_table']);
  const childCols   = pickExisting(list, ['child_cols', 'childcols', 'src_cols', 'from_cols', 'child_columns']);
  const parentSchema= pickExisting(list, ['parent_schema', 'parentschema', 'dst_schema', 'to_schema']);
  const parentTable = pickExisting(list, ['parent_table', 'parenttable', 'dst_table', 'to_table']);
  const parentCols  = pickExisting(list, ['parent_cols', 'parentcols', 'dst_cols', 'to_cols', 'parent_columns']);
  const fkName      = pickExisting(list, ['fk_name', 'fkname', 'constraint_name', 'conname']);

  // 1) Relationship mapping (best: all 6 fields)
  if (childSchema && childTable && childCols && parentSchema && parentTable && parentCols) {
    return {
      targetKind: 'relationship',
      confidence: 'high',
      reason: 'Found child_* and parent_* fields',
      mapping: {
        targetKind: 'relationship',
        fields: {
          childSchema,
          childTable,
          childCols,
          parentSchema,
          parentTable,
          parentCols,
          ...(fkName ? { fkName } : {}),
        },
      },
    };
  }

  // 2) Column mapping (schema+table+column)
  if (schemaCol && tableCol && columnCol) {
    return {
      targetKind: 'column',
      confidence: 'high',
      reason: 'Found schema+table+column fields',
      mapping: {
        targetKind: 'column',
        fields: { schema: schemaCol, table: tableCol, column: columnCol },
      },
    };
  }

  // 3) Table mapping (schema+table)
  if (schemaCol && tableCol) {
    return {
      targetKind: 'table',
      confidence: 'high',
      reason: 'Found schema+table fields',
      mapping: {
        targetKind: 'table',
        fields: { schema: schemaCol, table: tableCol },
      },
    };
  }

  // 4) Medium confidence fallbacks:
  // If table+column exists without schema -> still column-ish but weaker
  if (tableCol && columnCol) {
    return {
      targetKind: 'column',
      confidence: 'medium',
      reason: 'Found table+column but missing schema',
      mapping: {
        targetKind: 'column',
        fields: { schema: schemaCol ?? 'schema', table: tableCol, column: columnCol },
      },
    };
  }

  // If only table exists -> table-ish weak
  if (tableCol) {
    return {
      targetKind: 'table',
      confidence: 'low',
      reason: 'Found table field only',
      mapping: {
        targetKind: 'table',
        fields: { schema: schemaCol ?? 'schema', table: tableCol },
      },
    };
  }

  return { targetKind: null, mapping: null, confidence: 'none', reason: 'No mapping fields detected' };
}
```

---

# 2) SQL Test endpoint route (Fastify)

### `apps/api/src/routes/sqlTest.ts`

```ts
import type { FastifyInstance } from 'fastify';
import { z } from 'zod';
import { createTargetPool } from '../targetDb';
import { ensureSingleStatement, applyRowCap } from '../sqlrunner/limits';
import { suggestMapping } from '../sqlrunner/suggestMapping';

export async function sqlTestRoutes(app: FastifyInstance) {
  app.post('/api/v1/sql/test', async (req) => {
    const Body = z.object({
      targetDatabaseUrl: z.string().min(10),
      sql: z.string().min(1),
      rowCap: z.number().int().min(1).max(200).default(25),
      timeoutMs: z.number().int().min(250).max(20_000).default(2500),

      // optional: pass schemas for interpolation in future; v1 just informational
      schemas: z.array(z.string().min(1)).optional(),
      // optional: if true, do not auto-append LIMIT (still enforces rowCap via driver cap is not possible)
      noAutoLimit: z.boolean().optional().default(false),
    });

    const body = Body.parse(req.body);

    const pool = createTargetPool(body.targetDatabaseUrl);
    const client = await pool.connect();

    try {
      // 1) guardrails
      const single = ensureSingleStatement(body.sql);

      // 2) row cap (best-effort)
      const finalSql = body.noAutoLimit ? single : applyRowCap(single, body.rowCap);

      // 3) run with statement timeout scoped to this session
      await client.query('BEGIN');
      await client.query('SET LOCAL statement_timeout = $1', [body.timeoutMs]);

      const res = await client.query(finalSql);

      await client.query('COMMIT');

      const columns = (res.fields ?? []).map((f: any) => ({
        name: f.name,
        dataTypeId: f.dataTypeID,
      }));

      const rows = (res.rows ?? []).slice(0, body.rowCap);

      const suggestion = suggestMapping(columns);

      return {
        sqlFinal: finalSql,
        rows,
        columns,
        mappingSuggestions: suggestion,
      };
    } catch (e: any) {
      try { await client.query('ROLLBACK'); } catch {}
      // Keep message safe-ish, but useful
      return app.httpErrors.badRequest(e?.message ?? 'SQL test failed');
    } finally {
      client.release();
      await pool.end().catch(() => {});
    }
  });
}
```

---

# 3) Wire the route

### `apps/api/src/index.ts` (add)

```ts
import { sqlTestRoutes } from './routes/sqlTest';
// ...
await app.register(sqlTestRoutes);
```

---

# 4) Example request/response

### Request

```json
{
  "targetDatabaseUrl": "postgresql://user:pass@localhost:5432/db",
  "sql": "SELECT n.nspname AS schema, c.relname AS table FROM pg_class c JOIN pg_namespace n ON n.oid=c.relnamespace LIMIT 10",
  "rowCap": 25,
  "timeoutMs": 2500
}
```

### Response (shape)

```json
{
  "sqlFinal": "SELECT ... LIMIT 10",
  "columns": [
    {"name":"schema","dataTypeId":25},
    {"name":"table","dataTypeId":25}
  ],
  "rows": [
    {"schema":"public","table":"users"}
  ],
  "mappingSuggestions": {
    "targetKind": "table",
    "mapping": { "targetKind":"table", "fields": { "schema":"schema", "table":"table" } },
    "confidence": "high",
    "reason": "Found schema+table fields"
  }
}
```

---

# 5) Small upgrade you’ll want immediately (but optional)

If you want to support `{{schemas}}` and other tokens in the builder test too, we can:

* accept `schemas[]`
* run `interpolateSql()` before `ensureSingleStatement()`

Say **“2.3 sql/test with interpolation”** and I’ll patch the endpoint to safely interpolate using the same engine from 2.3 runner.

Here’s what’s left to make **`/api/v1/sql/test`** actually run cleanly in your backend (missing modules + hardening + how it plugs into the node builder + how we store mapping).

---

## 2.3 (cont.) — Required backend plumbing

# 1) Target DB connector (create a pool from `targetDatabaseUrl`)

### `apps/api/src/targetDb.ts`

```ts
import { Pool } from 'pg';

export function createTargetPool(targetDatabaseUrl: string) {
  // IMPORTANT: in production you’ll replace this with connectionId → secrets
  // v1: accept URL directly (dev only)
  return new Pool({
    connectionString: targetDatabaseUrl,
    max: 2, // keep small; these are transient pools
    idleTimeoutMillis: 5_000,
    connectionTimeoutMillis: 5_000,
  });
}
```

**Note:** In prod, don’t accept raw DB URLs from the client. Use `connectionId` + server-side secrets. For now it’s fine for internal dev.

---

# 2) Make sure `httpErrors` exists (Fastify plugin)

If you don’t already have it:

### `apps/api/src/plugins/httpErrors.ts`

```ts
import fp from 'fastify-plugin';
import fastifySensible from '@fastify/sensible';

export default fp(async (app) => {
  await app.register(fastifySensible);
});
```

Then in `apps/api/src/index.ts`:

```ts
import httpErrors from './plugins/httpErrors';
await app.register(httpErrors);
```

And install:

```bash
pnpm --filter api add @fastify/sensible fastify-plugin
```

---

# 3) Add a “read-only SQL” guard (recommended v1)

Right now, `ensureSingleStatement()` blocks multiple statements, but a single `DELETE` is still possible.

Add a simple “SELECT-only” guard for `/sql/test` (builder should not mutate data).

### `apps/api/src/sqlrunner/readonly.ts`

```ts
export class SqlReadOnlyError extends Error {
  code = 'SQL_READONLY_ERROR';
}

export function ensureReadOnly(sql: string) {
  const s = sql.trim().toLowerCase();

  // allow WITH ... SELECT ...
  const isSelecty = s.startsWith('select') || s.startsWith('with');

  if (!isSelecty) throw new SqlReadOnlyError('Only SELECT/WITH queries are allowed in SQL test');

  // deny common write keywords anywhere
  const banned = [
    'insert ', 'update ', 'delete ', 'drop ', 'alter ', 'truncate ',
    'create ', 'grant ', 'revoke ', 'comment ', 'vacuum ', 'analyze ',
    'call ', 'do ', 'execute ', 'refresh ', 'set ',
  ];

  for (const kw of banned) {
    if (s.includes(kw)) throw new SqlReadOnlyError(`Disallowed keyword detected: ${kw.trim()}`);
  }

  return sql;
}
```

Patch `sqlTest.ts`:

```ts
import { ensureReadOnly } from '../sqlrunner/readonly';

const single = ensureSingleStatement(body.sql);
const ro = ensureReadOnly(single);
const finalSql = body.noAutoLimit ? ro : applyRowCap(ro, body.rowCap);
```

---

# 4) (Optional) Interpolation support in `/sql/test` (builder)

This is the most useful next improvement because it lets users test real templates:

### Patch `apps/api/src/routes/sqlTest.ts` (add)

```ts
import { interpolateSql } from '../sqlrunner/interpolate';
```

Then before `ensureSingleStatement`:

```ts
const ctx = {
  schemas: body.schemas ?? [],
  thresholds: body.thresholds ?? {},
  vars: body.vars ?? {},
};

const interpolated = interpolateSql(body.sql, ctx);
const single = ensureSingleStatement(interpolated);
const ro = ensureReadOnly(single);
const finalSql = body.noAutoLimit ? ro : applyRowCap(ro, body.rowCap);
```

Update Body schema to accept:

```ts
thresholds: z.record(z.number()).optional(),
vars: z.record(z.any()).optional(),
```

---

## 2.3 (cont.) — Builder workflow (how UI uses this)

### What frontend does when user writes SQL

1. User enters SQL
2. UI calls `POST /api/v1/sql/test`
3. UI displays:

   * `columns[]` + `rows[]`
   * `mappingSuggestions`
4. User can click “Use suggestion” which writes `result_mapping` onto the checklist node.

### What gets saved on the node

`checklist_nodes.result_mapping` = `mappingSuggestions.mapping`

Example:

```json
{
  "targetKind": "table",
  "fields": { "schema": "schema", "table": "table" }
}
```

---

## 2.3 (cont.) — What’s still missing to “ship”

You now have:

* SQL test endpoint ✅
* SQL runner engine ✅

Remaining backend pieces to fully close the loop:

### 1) Node PATCH must allow saving `result_mapping`

Make sure `PATCH /api/v1/checklist-nodes/:nodeId` supports:

* `sql_template`
* `pass_fail_rule`
* `result_mapping`

### 2) `/run` must execute SQL checks per node scope (next big step)

Right now SQL runner uses ctx `{schemas}` only.
To be “real” you’ll want iteration modes:

* **diagram scope**: run once with `schemas`
* **table scope**: run once per table (ctx includes `schema` + `table`)
* **schema scope**: run once per schema (ctx includes `schema`)

This is the next upgrade that unlocks more powerful checks.

---

## Next code bundle (pick)

If you say one of these, I’ll drop the full code:

1. **“2.3 scope iteration”**

   * table iteration (pull table list from snapshot/introspection)
   * runs sql template per table/schema
   * aggregates results into per-item outputs

2. **“2.1 nodes patch route code”**

   * full Fastify PATCH handler validating item fields
   * persists `sql_template`, `pass_fail_rule`, `result_mapping`

3. **“2.2 issues filters + sorting”**

   * implement query filters: severity/section/schema/table
   * blocking-first sort


## 2.3 (cont.) — Required backend plumbing

# 1) Target DB connector (create a pool from `targetDatabaseUrl`)

### `apps/api/src/targetDb.ts`

```ts
import { Pool } from 'pg';

export function createTargetPool(targetDatabaseUrl: string) {
  // IMPORTANT: in production you’ll replace this with connectionId → secrets
  // v1: accept URL directly (dev only)
  return new Pool({
    connectionString: targetDatabaseUrl,
    max: 2, // keep small; these are transient pools
    idleTimeoutMillis: 5_000,
    connectionTimeoutMillis: 5_000,
  });
}
```

**Note:** In prod, don’t accept raw DB URLs from the client. Use `connectionId` + server-side secrets. For now it’s fine for internal dev.

---

# 2) Make sure `httpErrors` exists (Fastify plugin)

If you don’t already have it:

### `apps/api/src/plugins/httpErrors.ts`

```ts
import fp from 'fastify-plugin';
import fastifySensible from '@fastify/sensible';

export default fp(async (app) => {
  await app.register(fastifySensible);
});
```

Then in `apps/api/src/index.ts`:

```ts
import httpErrors from './plugins/httpErrors';
await app.register(httpErrors);
```

And install:

```bash
pnpm --filter api add @fastify/sensible fastify-plugin
```

---

# 3) Add a “read-only SQL” guard (recommended v1)

Right now, `ensureSingleStatement()` blocks multiple statements, but a single `DELETE` is still possible.

Add a simple “SELECT-only” guard for `/sql/test` (builder should not mutate data).

### `apps/api/src/sqlrunner/readonly.ts`

```ts
export class SqlReadOnlyError extends Error {
  code = 'SQL_READONLY_ERROR';
}

export function ensureReadOnly(sql: string) {
  const s = sql.trim().toLowerCase();

  // allow WITH ... SELECT ...
  const isSelecty = s.startsWith('select') || s.startsWith('with');

  if (!isSelecty) throw new SqlReadOnlyError('Only SELECT/WITH queries are allowed in SQL test');

  // deny common write keywords anywhere
  const banned = [
    'insert ', 'update ', 'delete ', 'drop ', 'alter ', 'truncate ',
    'create ', 'grant ', 'revoke ', 'comment ', 'vacuum ', 'analyze ',
    'call ', 'do ', 'execute ', 'refresh ', 'set ',
  ];

  for (const kw of banned) {
    if (s.includes(kw)) throw new SqlReadOnlyError(`Disallowed keyword detected: ${kw.trim()}`);
  }

  return sql;
}
```

Patch `sqlTest.ts`:

```ts
import { ensureReadOnly } from '../sqlrunner/readonly';

const single = ensureSingleStatement(body.sql);
const ro = ensureReadOnly(single);
const finalSql = body.noAutoLimit ? ro : applyRowCap(ro, body.rowCap);
```

---

# 4) (Optional) Interpolation support in `/sql/test` (builder)

This is the most useful next improvement because it lets users test real templates:

### Patch `apps/api/src/routes/sqlTest.ts` (add)

```ts
import { interpolateSql } from '../sqlrunner/interpolate';
```

Then before `ensureSingleStatement`:

```ts
const ctx = {
  schemas: body.schemas ?? [],
  thresholds: body.thresholds ?? {},
  vars: body.vars ?? {},
};

const interpolated = interpolateSql(body.sql, ctx);
const single = ensureSingleStatement(interpolated);
const ro = ensureReadOnly(single);
const finalSql = body.noAutoLimit ? ro : applyRowCap(ro, body.rowCap);
```

Update Body schema to accept:

```ts
thresholds: z.record(z.number()).optional(),
vars: z.record(z.any()).optional(),
```

---

## 2.3 (cont.) — Builder workflow (how UI uses this)

### What frontend does when user writes SQL

1. User enters SQL
2. UI calls `POST /api/v1/sql/test`
3. UI displays:

   * `columns[]` + `rows[]`
   * `mappingSuggestions`
4. User can click “Use suggestion” which writes `result_mapping` onto the checklist node.

### What gets saved on the node

`checklist_nodes.result_mapping` = `mappingSuggestions.mapping`

Example:

```json
{
  "targetKind": "table",
  "fields": { "schema": "schema", "table": "table" }
}
```

---

## 2.3 (cont.) — What’s still missing to “ship”

You now have:

* SQL test endpoint ✅
* SQL runner engine ✅

Remaining backend pieces to fully close the loop:

### 1) Node PATCH must allow saving `result_mapping`

Make sure `PATCH /api/v1/checklist-nodes/:nodeId` supports:

* `sql_template`
* `pass_fail_rule`
* `result_mapping`

### 2) `/run` must execute SQL checks per node scope (next big step)

Right now SQL runner uses ctx `{schemas}` only.
To be “real” you’ll want iteration modes:

* **diagram scope**: run once with `schemas`
* **table scope**: run once per table (ctx includes `schema` + `table`)
* **schema scope**: run once per schema (ctx includes `schema`)

This is the next upgrade that unlocks more powerful checks.

---

Here’s what’s left to make **`/api/v1/sql/test`** actually run cleanly in your backend (missing modules + hardening + how it plugs into the node builder + how we store mapping).

---

## 2.3 (cont.) — Required backend plumbing

# 1) Target DB connector (create a pool from `targetDatabaseUrl`)

### `apps/api/src/targetDb.ts`

```ts
import { Pool } from 'pg';

export function createTargetPool(targetDatabaseUrl: string) {
  // IMPORTANT: in production you’ll replace this with connectionId → secrets
  // v1: accept URL directly (dev only)
  return new Pool({
    connectionString: targetDatabaseUrl,
    max: 2, // keep small; these are transient pools
    idleTimeoutMillis: 5_000,
    connectionTimeoutMillis: 5_000,
  });
}
```

**Note:** In prod, don’t accept raw DB URLs from the client. Use `connectionId` + server-side secrets. For now it’s fine for internal dev.

---

# 2) Make sure `httpErrors` exists (Fastify plugin)

If you don’t already have it:

### `apps/api/src/plugins/httpErrors.ts`

```ts
import fp from 'fastify-plugin';
import fastifySensible from '@fastify/sensible';

export default fp(async (app) => {
  await app.register(fastifySensible);
});
```

Then in `apps/api/src/index.ts`:

```ts
import httpErrors from './plugins/httpErrors';
await app.register(httpErrors);
```

And install:

```bash
pnpm --filter api add @fastify/sensible fastify-plugin
```

---

Right now, `ensureSingleStatement()` blocks multiple statements, but a single `DELETE` is still possible.

Add a simple “SELECT-only” guard for `/sql/test` (builder should not mutate data).

### `apps/api/src/sqlrunner/readonly.ts`

```ts
export class SqlReadOnlyError extends Error {
  code = 'SQL_READONLY_ERROR';
}

export function ensureReadOnly(sql: string) {
  const s = sql.trim().toLowerCase();

  // allow WITH ... SELECT ...
  const isSelecty = s.startsWith('select') || s.startsWith('with');

  if (!isSelecty) throw new SqlReadOnlyError('Only SELECT/WITH queries are allowed in SQL test');

  // deny common write keywords anywhere
  const banned = [
    'insert ', 'update ', 'delete ', 'drop ', 'alter ', 'truncate ',
    'create ', 'grant ', 'revoke ', 'comment ', 'vacuum ', 'analyze ',
    'call ', 'do ', 'execute ', 'refresh ', 'set ',
  ];

  for (const kw of banned) {
    if (s.includes(kw)) throw new SqlReadOnlyError(`Disallowed keyword detected: ${kw.trim()}`);
  }

  return sql;
}
```

Patch `sqlTest.ts`:

```ts
import { ensureReadOnly } from '../sqlrunner/readonly';

const single = ensureSingleStatement(body.sql);
const ro = ensureReadOnly(single);
const finalSql = body.noAutoLimit ? ro : applyRowCap(ro, body.rowCap);
```

---

# 4) (Optional) Interpolation support in `/sql/test` (builder)

This is the most useful next improvement because it lets users test real templates:

### Patch `apps/api/src/routes/sqlTest.ts` (add)

```ts
import { interpolateSql } from '../sqlrunner/interpolate';
```

Then before `ensureSingleStatement`:

```ts
const ctx = {
  schemas: body.schemas ?? [],
  thresholds: body.thresholds ?? {},
  vars: body.vars ?? {},
};

const interpolated = interpolateSql(body.sql, ctx);
const single = ensureSingleStatement(interpolated);
const ro = ensureReadOnly(single);
const finalSql = body.noAutoLimit ? ro : applyRowCap(ro, body.rowCap);
```

Update Body schema to accept:

```ts
thresholds: z.record(z.number()).optional(),
vars: z.record(z.any()).optional(),
```

---

## 2.3 (cont.) — Builder workflow (how UI uses this)

### What frontend does when user writes SQL

1. User enters SQL
2. UI calls `POST /api/v1/sql/test`
3. UI displays:

   * `columns[]` + `rows[]`
   * `mappingSuggestions`
4. User can click “Use suggestion” which writes `result_mapping` onto the checklist node.

### What gets saved on the node

`checklist_nodes.result_mapping` = `mappingSuggestions.mapping`

Example:

```json
{
  "targetKind": "table",
  "fields": { "schema": "schema", "table": "table" }
}
```

---

## 2.3 (cont.) — What’s still missing to “ship”

You now have:

* SQL test endpoint ✅
* SQL runner engine ✅

Remaining backend pieces to fully close the loop:

### 1) Node PATCH must allow saving `result_mapping`

Make sure `PATCH /api/v1/checklist-nodes/:nodeId` supports:

* `sql_template`
* `pass_fail_rule`
* `result_mapping`

### 2) `/run` must execute SQL checks per node scope (next big step)

Right now SQL runner uses ctx `{schemas}` only.
To be “real” you’ll want iteration modes:

* **diagram scope**: run once with `schemas`
* **table scope**: run once per table (ctx includes `schema` + `table`)
* **schema scope**: run once per schema (ctx includes `schema`)

This is the next upgrade that unlocks more powerful checks.

---

## Next code bundle (pick)

If you say one of these, I’ll drop the full code:

1. **“2.3 scope iteration”**

   * table iteration (pull table list from snapshot/introspection)
   * runs sql template per table/schema
   * aggregates results into per-item outputs

2. **“2.1 nodes patch route code”**

   * full Fastify PATCH handler validating item fields
   * persists `sql_template`, `pass_fail_rule`, `result_mapping`

3. **“2.2 issues filters + sorting”**

   * implement query filters: severity/section/schema/table
   * blocking-first sort

If you don’t choose, the best next step is **(1) 2.3 scope iteration** because it turns SQL checks from “demo” into “product.”

Here’s **Section Labels** (a clean “section path” for each issue), implemented in a way that’s fast and UI-friendly.

You’ll get for every issue:

* `sectionPath`: `[{id,title}]` from root → … → parent section/group → item
* `sectionTitle`: a single string like `"Schema → Keys → Foreign Keys"`

---

## 1) Add section-path CTE and return it per issue

### Patch `apps/api/src/routes/issues.ts`

Replace the big `sql = ...` with this version (same filters/sorting as before, but now includes `section_path`):

```ts
const sql = `
  WITH
  inst AS (
    SELECT template_version_id
    FROM checklist_instances
    WHERE id = $1
  ),

  -- Base filtered results (keeps your existing WHERE + ORDER + LIMIT)
  base AS (
    SELECT
      r.id AS result_id,
      r.node_id,
      r.status AS result_status,
      r.severity AS result_severity,
      r.output_summary,
      r.output_stats,
      r.target_ref,
      r.updated_at,

      n.title AS node_title,
      n.check_ref,
      n.node_type,
      n.parent_id

    FROM checklist_instance_results r
    JOIN checklist_nodes n ON n.id = r.node_id
    WHERE ${where.join('\n AND ')}

    ORDER BY
      CASE r.severity
        WHEN 'blocking' THEN 0
        WHEN 'error' THEN 1
        WHEN 'warning' THEN 2
        ELSE 9
      END ASC,
      CASE r.status
        WHEN 'blocked' THEN 0
        WHEN 'fail' THEN 1
        WHEN 'warning' THEN 2
        WHEN 'unchecked' THEN 3
        WHEN 'pass' THEN 4
        ELSE 9
      END ASC,
      r.updated_at DESC

    LIMIT ${q.limit} OFFSET ${q.offset}
  ),

  -- Walk upwards from each base.node_id to the root to build a path
  chain AS (
    SELECT
      b.result_id,
      n.id,
      n.parent_id,
      n.title,
      n.node_type,
      0 AS depth
    FROM base b
    JOIN checklist_nodes n ON n.id = b.node_id

    UNION ALL

    SELECT
      c.result_id,
      p.id,
      p.parent_id,
      p.title,
      p.node_type,
      c.depth + 1
    FROM chain c
    JOIN checklist_nodes p ON p.id = c.parent_id
    WHERE c.parent_id IS NOT NULL
  ),

  -- Aggregate each chain into an ordered JSON array (root → leaf)
  paths AS (
    SELECT
      result_id,
      jsonb_agg(
        jsonb_build_object(
          'id', id,
          'title', title,
          'nodeType', node_type
        )
        ORDER BY depth DESC
      ) AS section_path
    FROM chain
    GROUP BY result_id
  )

  SELECT
    b.*,
    COALESCE(p.section_path, '[]'::jsonb) AS section_path
  FROM base b
  LEFT JOIN paths p ON p.result_id = b.result_id
`;
```

---

## 2) Return `sectionPath` + `sectionTitle` in the response

Patch the mapper in `issues.ts`:

```ts
const issues = rows.map((r: any) => {
  const sectionPath = (r.section_path ?? []) as Array<{ id: string; title: string; nodeType: string }>;

  // Turn it into a nice label string; optionally exclude the leaf item title if you want
  const sectionTitle = sectionPath
    .filter((x) => x.nodeType !== 'item') // keep only section/group nodes for label
    .map((x) => x.title)
    .join(' → ');

  return {
    resultId: r.result_id,
    nodeId: r.node_id,
    code: r.check_ref ?? 'SQL',
    title: r.node_title,
    severity: r.result_severity,
    status: r.result_status,
    updatedAt: r.updated_at,
    outputSummary: r.output_summary ?? null,
    outputStats: r.output_stats ?? null,
    targetRef: r.target_ref ?? null,
    focus: focusFromTargetRef(r.target_ref),

    sectionPath,
    sectionTitle: sectionTitle || null,
  };
});
```

---

## 3) Optional: also return `itemTitle` separately (nice for UI)

If your UI wants:

* `sectionTitle` = `"Schema → Keys"`
* `itemTitle` = `"FK columns missing index"`

Add:

```ts
const itemTitle = sectionPath.length ? sectionPath[sectionPath.length - 1]?.title : r.node_title;
```

---

## 4) Performance notes (so this scales)

* This approach computes paths only for the **paged `base` results**, not for the whole instance → stays fast even with thousands of nodes.
* Make sure you have indexes:

  * `checklist_instance_results(instance_id, severity, status, updated_at)`
  * `checklist_nodes(id)` is PK, plus `checklist_nodes(parent_id)` index helps recursion.

---

If you want the UI-ready “breadcrumb” formatting too (with clickable parents), I can add a tiny helper that converts `sectionPath` into a **breadcrumb array** where every crumb has a `nodeId` so the frontend can open that subtree in the checklist panel.
