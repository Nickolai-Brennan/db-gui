Below is **2.3 SQL test endpoint code** (Fastify + pg) that supports your SQL Builder:

✅ runs SQL against a **target DB** (via `targetDatabaseUrl`)
✅ **safe limits**: single statement, timeout, row cap, auto LIMIT
✅ returns **sample rows + column metadata**
✅ returns **mapping suggestions** (`table|column|relationship`) based on result columns
✅ (optional) returns `sqlFinal` so UI can show what actually ran

---

# 1) Mapping suggestion heuristics

### `apps/api/src/sqlrunner/suggestMapping.ts`

```ts
import type { ResultMapping, TargetKind } from './mapping';

type Col = { name: string; dataTypeId?: number };

function lc(s: string) {
  return s.trim().toLowerCase();
}

function has(cols: string[], ...cands: string[]) {
  const set = new Set(cols);
  return cands.every((c) => set.has(c));
}

function pickExisting(cols: string[], options: string[]) {
  const set = new Set(cols);
  for (const o of options) if (set.has(o)) return o;
  return null;
}

export type MappingSuggestion = {
  targetKind: TargetKind | null;
  mapping: ResultMapping | null;
  confidence: 'high' | 'medium' | 'low' | 'none';
  reason: string;
};

export function suggestMapping(columns: Col[]): MappingSuggestion {
  const cols = columns.map((c) => lc(c.name));
  const set = new Set(cols);
  const list = Array.from(set);

  // Common field aliases
  const schemaCol = pickExisting(list, ['schema', 'schema_name', 'nspname', 'table_schema']);
  const tableCol = pickExisting(list, ['table', 'table_name', 'relname', 'table_name']);
  const columnCol = pickExisting(list, ['column', 'column_name', 'attname']);

  // Relationship candidates
  const childSchema = pickExisting(list, ['child_schema', 'childschema', 'src_schema', 'from_schema']);
  const childTable  = pickExisting(list, ['child_table', 'childtable', 'src_table', 'from_table']);
  const childCols   = pickExisting(list, ['child_cols', 'childcols', 'src_cols', 'from_cols', 'child_columns']);
  const parentSchema= pickExisting(list, ['parent_schema', 'parentschema', 'dst_schema', 'to_schema']);
  const parentTable = pickExisting(list, ['parent_table', 'parenttable', 'dst_table', 'to_table']);
  const parentCols  = pickExisting(list, ['parent_cols', 'parentcols', 'dst_cols', 'to_cols', 'parent_columns']);
  const fkName      = pickExisting(list, ['fk_name', 'fkname', 'constraint_name', 'conname']);

  // 1) Relationship mapping (best: all 6 fields)
  if (childSchema && childTable && childCols && parentSchema && parentTable && parentCols) {
    return {
      targetKind: 'relationship',
      confidence: 'high',
      reason: 'Found child_* and parent_* fields',
      mapping: {
        targetKind: 'relationship',
        fields: {
          childSchema,
          childTable,
          childCols,
          parentSchema,
          parentTable,
          parentCols,
          ...(fkName ? { fkName } : {}),
        },
      },
    };
  }

  // 2) Column mapping (schema+table+column)
  if (schemaCol && tableCol && columnCol) {
    return {
      targetKind: 'column',
      confidence: 'high',
      reason: 'Found schema+table+column fields',
      mapping: {
        targetKind: 'column',
        fields: { schema: schemaCol, table: tableCol, column: columnCol },
      },
    };
  }

  // 3) Table mapping (schema+table)
  if (schemaCol && tableCol) {
    return {
      targetKind: 'table',
      confidence: 'high',
      reason: 'Found schema+table fields',
      mapping: {
        targetKind: 'table',
        fields: { schema: schemaCol, table: tableCol },
      },
    };
  }

  // 4) Medium confidence fallbacks:
  // If table+column exists without schema -> still column-ish but weaker
  if (tableCol && columnCol) {
    return {
      targetKind: 'column',
      confidence: 'medium',
      reason: 'Found table+column but missing schema',
      mapping: {
        targetKind: 'column',
        fields: { schema: schemaCol ?? 'schema', table: tableCol, column: columnCol },
      },
    };
  }

  // If only table exists -> table-ish weak
  if (tableCol) {
    return {
      targetKind: 'table',
      confidence: 'low',
      reason: 'Found table field only',
      mapping: {
        targetKind: 'table',
        fields: { schema: schemaCol ?? 'schema', table: tableCol },
      },
    };
  }

  return { targetKind: null, mapping: null, confidence: 'none', reason: 'No mapping fields detected' };
}
```

---

# 2) SQL Test endpoint route (Fastify)

### `apps/api/src/routes/sqlTest.ts`

```ts
import type { FastifyInstance } from 'fastify';
import { z } from 'zod';
import { createTargetPool } from '../targetDb';
import { ensureSingleStatement, applyRowCap } from '../sqlrunner/limits';
import { suggestMapping } from '../sqlrunner/suggestMapping';

export async function sqlTestRoutes(app: FastifyInstance) {
  app.post('/api/v1/sql/test', async (req) => {
    const Body = z.object({
      targetDatabaseUrl: z.string().min(10),
      sql: z.string().min(1),
      rowCap: z.number().int().min(1).max(200).default(25),
      timeoutMs: z.number().int().min(250).max(20_000).default(2500),

      // optional: pass schemas for interpolation in future; v1 just informational
      schemas: z.array(z.string().min(1)).optional(),
      // optional: if true, do not auto-append LIMIT (still enforces rowCap via driver cap is not possible)
      noAutoLimit: z.boolean().optional().default(false),
    });

    const body = Body.parse(req.body);

    const pool = createTargetPool(body.targetDatabaseUrl);
    const client = await pool.connect();

    try {
      // 1) guardrails
      const single = ensureSingleStatement(body.sql);

      // 2) row cap (best-effort)
      const finalSql = body.noAutoLimit ? single : applyRowCap(single, body.rowCap);

      // 3) run with statement timeout scoped to this session
      await client.query('BEGIN');
      await client.query('SET LOCAL statement_timeout = $1', [body.timeoutMs]);

      const res = await client.query(finalSql);

      await client.query('COMMIT');

      const columns = (res.fields ?? []).map((f: any) => ({
        name: f.name,
        dataTypeId: f.dataTypeID,
      }));

      const rows = (res.rows ?? []).slice(0, body.rowCap);

      const suggestion = suggestMapping(columns);

      return {
        sqlFinal: finalSql,
        rows,
        columns,
        mappingSuggestions: suggestion,
      };
    } catch (e: any) {
      try { await client.query('ROLLBACK'); } catch {}
      // Keep message safe-ish, but useful
      return app.httpErrors.badRequest(e?.message ?? 'SQL test failed');
    } finally {
      client.release();
      await pool.end().catch(() => {});
    }
  });
}
```

---

# 3) Wire the route

### `apps/api/src/index.ts` (add)

```ts
import { sqlTestRoutes } from './routes/sqlTest';
// ...
await app.register(sqlTestRoutes);
```

---

# 4) Example request/response

### Request

```json
{
  "targetDatabaseUrl": "postgresql://user:pass@localhost:5432/db",
  "sql": "SELECT n.nspname AS schema, c.relname AS table FROM pg_class c JOIN pg_namespace n ON n.oid=c.relnamespace LIMIT 10",
  "rowCap": 25,
  "timeoutMs": 2500
}
```

### Response (shape)

```json
{
  "sqlFinal": "SELECT ... LIMIT 10",
  "columns": [
    {"name":"schema","dataTypeId":25},
    {"name":"table","dataTypeId":25}
  ],
  "rows": [
    {"schema":"public","table":"users"}
  ],
  "mappingSuggestions": {
    "targetKind": "table",
    "mapping": { "targetKind":"table", "fields": { "schema":"schema", "table":"table" } },
    "confidence": "high",
    "reason": "Found schema+table fields"
  }
}
```

---

# 5) Small upgrade you’ll want immediately (but optional)

If you want to support `{{schemas}}` and other tokens in the builder test too, we can:

* accept `schemas[]`
* run `interpolateSql()` before `ensureSingleStatement()`

Say **“2.3 sql/test with interpolation”** and I’ll patch the endpoint to safely interpolate using the same engine from 2.3 runner.

Here’s what’s left to make **`/api/v1/sql/test`** actually run cleanly in your backend (missing modules + hardening + how it plugs into the node builder + how we store mapping).

---

## 2.3 (cont.) — Required backend plumbing

# 1) Target DB connector (create a pool from `targetDatabaseUrl`)

### `apps/api/src/targetDb.ts`

```ts
import { Pool } from 'pg';

export function createTargetPool(targetDatabaseUrl: string) {
  // IMPORTANT: in production you’ll replace this with connectionId → secrets
  // v1: accept URL directly (dev only)
  return new Pool({
    connectionString: targetDatabaseUrl,
    max: 2, // keep small; these are transient pools
    idleTimeoutMillis: 5_000,
    connectionTimeoutMillis: 5_000,
  });
}
```

**Note:** In prod, don’t accept raw DB URLs from the client. Use `connectionId` + server-side secrets. For now it’s fine for internal dev.

---

# 2) Make sure `httpErrors` exists (Fastify plugin)

If you don’t already have it:

### `apps/api/src/plugins/httpErrors.ts`

```ts
import fp from 'fastify-plugin';
import fastifySensible from '@fastify/sensible';

export default fp(async (app) => {
  await app.register(fastifySensible);
});
```

Then in `apps/api/src/index.ts`:

```ts
import httpErrors from './plugins/httpErrors';
await app.register(httpErrors);
```

And install:

```bash
pnpm --filter api add @fastify/sensible fastify-plugin
```

---

# 3) Add a “read-only SQL” guard (recommended v1)

Right now, `ensureSingleStatement()` blocks multiple statements, but a single `DELETE` is still possible.

Add a simple “SELECT-only” guard for `/sql/test` (builder should not mutate data).

### `apps/api/src/sqlrunner/readonly.ts`

```ts
export class SqlReadOnlyError extends Error {
  code = 'SQL_READONLY_ERROR';
}

export function ensureReadOnly(sql: string) {
  const s = sql.trim().toLowerCase();

  // allow WITH ... SELECT ...
  const isSelecty = s.startsWith('select') || s.startsWith('with');

  if (!isSelecty) throw new SqlReadOnlyError('Only SELECT/WITH queries are allowed in SQL test');

  // deny common write keywords anywhere
  const banned = [
    'insert ', 'update ', 'delete ', 'drop ', 'alter ', 'truncate ',
    'create ', 'grant ', 'revoke ', 'comment ', 'vacuum ', 'analyze ',
    'call ', 'do ', 'execute ', 'refresh ', 'set ',
  ];

  for (const kw of banned) {
    if (s.includes(kw)) throw new SqlReadOnlyError(`Disallowed keyword detected: ${kw.trim()}`);
  }

  return sql;
}
```

Patch `sqlTest.ts`:

```ts
import { ensureReadOnly } from '../sqlrunner/readonly';

const single = ensureSingleStatement(body.sql);
const ro = ensureReadOnly(single);
const finalSql = body.noAutoLimit ? ro : applyRowCap(ro, body.rowCap);
```

---

# 4) (Optional) Interpolation support in `/sql/test` (builder)

This is the most useful next improvement because it lets users test real templates:

### Patch `apps/api/src/routes/sqlTest.ts` (add)

```ts
import { interpolateSql } from '../sqlrunner/interpolate';
```

Then before `ensureSingleStatement`:

```ts
const ctx = {
  schemas: body.schemas ?? [],
  thresholds: body.thresholds ?? {},
  vars: body.vars ?? {},
};

const interpolated = interpolateSql(body.sql, ctx);
const single = ensureSingleStatement(interpolated);
const ro = ensureReadOnly(single);
const finalSql = body.noAutoLimit ? ro : applyRowCap(ro, body.rowCap);
```

Update Body schema to accept:

```ts
thresholds: z.record(z.number()).optional(),
vars: z.record(z.any()).optional(),
```

---

## 2.3 (cont.) — Builder workflow (how UI uses this)

### What frontend does when user writes SQL

1. User enters SQL
2. UI calls `POST /api/v1/sql/test`
3. UI displays:

   * `columns[]` + `rows[]`
   * `mappingSuggestions`
4. User can click “Use suggestion” which writes `result_mapping` onto the checklist node.

### What gets saved on the node

`checklist_nodes.result_mapping` = `mappingSuggestions.mapping`

Example:

```json
{
  "targetKind": "table",
  "fields": { "schema": "schema", "table": "table" }
}
```

---

## 2.3 (cont.) — What’s still missing to “ship”

You now have:

* SQL test endpoint ✅
* SQL runner engine ✅

Remaining backend pieces to fully close the loop:

### 1) Node PATCH must allow saving `result_mapping`

Make sure `PATCH /api/v1/checklist-nodes/:nodeId` supports:

* `sql_template`
* `pass_fail_rule`
* `result_mapping`

### 2) `/run` must execute SQL checks per node scope (next big step)

Right now SQL runner uses ctx `{schemas}` only.
To be “real” you’ll want iteration modes:

* **diagram scope**: run once with `schemas`
* **table scope**: run once per table (ctx includes `schema` + `table`)
* **schema scope**: run once per schema (ctx includes `schema`)

This is the next upgrade that unlocks more powerful checks.

---

## Next code bundle (pick)

If you say one of these, I’ll drop the full code:

1. **“2.3 scope iteration”**

   * table iteration (pull table list from snapshot/introspection)
   * runs sql template per table/schema
   * aggregates results into per-item outputs

2. **“2.1 nodes patch route code”**

   * full Fastify PATCH handler validating item fields
   * persists `sql_template`, `pass_fail_rule`, `result_mapping`

3. **“2.2 issues filters + sorting”**

   * implement query filters: severity/section/schema/table
   * blocking-first sort


## 2.3 (cont.) — Required backend plumbing

# 1) Target DB connector (create a pool from `targetDatabaseUrl`)

### `apps/api/src/targetDb.ts`

```ts
import { Pool } from 'pg';

export function createTargetPool(targetDatabaseUrl: string) {
  // IMPORTANT: in production you’ll replace this with connectionId → secrets
  // v1: accept URL directly (dev only)
  return new Pool({
    connectionString: targetDatabaseUrl,
    max: 2, // keep small; these are transient pools
    idleTimeoutMillis: 5_000,
    connectionTimeoutMillis: 5_000,
  });
}
```

**Note:** In prod, don’t accept raw DB URLs from the client. Use `connectionId` + server-side secrets. For now it’s fine for internal dev.

---

# 2) Make sure `httpErrors` exists (Fastify plugin)

If you don’t already have it:

### `apps/api/src/plugins/httpErrors.ts`

```ts
import fp from 'fastify-plugin';
import fastifySensible from '@fastify/sensible';

export default fp(async (app) => {
  await app.register(fastifySensible);
});
```

Then in `apps/api/src/index.ts`:

```ts
import httpErrors from './plugins/httpErrors';
await app.register(httpErrors);
```

And install:

```bash
pnpm --filter api add @fastify/sensible fastify-plugin
```

---

# 3) Add a “read-only SQL” guard (recommended v1)

Right now, `ensureSingleStatement()` blocks multiple statements, but a single `DELETE` is still possible.

Add a simple “SELECT-only” guard for `/sql/test` (builder should not mutate data).

### `apps/api/src/sqlrunner/readonly.ts`

```ts
export class SqlReadOnlyError extends Error {
  code = 'SQL_READONLY_ERROR';
}

export function ensureReadOnly(sql: string) {
  const s = sql.trim().toLowerCase();

  // allow WITH ... SELECT ...
  const isSelecty = s.startsWith('select') || s.startsWith('with');

  if (!isSelecty) throw new SqlReadOnlyError('Only SELECT/WITH queries are allowed in SQL test');

  // deny common write keywords anywhere
  const banned = [
    'insert ', 'update ', 'delete ', 'drop ', 'alter ', 'truncate ',
    'create ', 'grant ', 'revoke ', 'comment ', 'vacuum ', 'analyze ',
    'call ', 'do ', 'execute ', 'refresh ', 'set ',
  ];

  for (const kw of banned) {
    if (s.includes(kw)) throw new SqlReadOnlyError(`Disallowed keyword detected: ${kw.trim()}`);
  }

  return sql;
}
```

Patch `sqlTest.ts`:

```ts
import { ensureReadOnly } from '../sqlrunner/readonly';

const single = ensureSingleStatement(body.sql);
const ro = ensureReadOnly(single);
const finalSql = body.noAutoLimit ? ro : applyRowCap(ro, body.rowCap);
```

---

# 4) (Optional) Interpolation support in `/sql/test` (builder)

This is the most useful next improvement because it lets users test real templates:

### Patch `apps/api/src/routes/sqlTest.ts` (add)

```ts
import { interpolateSql } from '../sqlrunner/interpolate';
```

Then before `ensureSingleStatement`:

```ts
const ctx = {
  schemas: body.schemas ?? [],
  thresholds: body.thresholds ?? {},
  vars: body.vars ?? {},
};

const interpolated = interpolateSql(body.sql, ctx);
const single = ensureSingleStatement(interpolated);
const ro = ensureReadOnly(single);
const finalSql = body.noAutoLimit ? ro : applyRowCap(ro, body.rowCap);
```

Update Body schema to accept:

```ts
thresholds: z.record(z.number()).optional(),
vars: z.record(z.any()).optional(),
```

---

## 2.3 (cont.) — Builder workflow (how UI uses this)

### What frontend does when user writes SQL

1. User enters SQL
2. UI calls `POST /api/v1/sql/test`
3. UI displays:

   * `columns[]` + `rows[]`
   * `mappingSuggestions`
4. User can click “Use suggestion” which writes `result_mapping` onto the checklist node.

### What gets saved on the node

`checklist_nodes.result_mapping` = `mappingSuggestions.mapping`

Example:

```json
{
  "targetKind": "table",
  "fields": { "schema": "schema", "table": "table" }
}
```

---

## 2.3 (cont.) — What’s still missing to “ship”

You now have:

* SQL test endpoint ✅
* SQL runner engine ✅

Remaining backend pieces to fully close the loop:

### 1) Node PATCH must allow saving `result_mapping`

Make sure `PATCH /api/v1/checklist-nodes/:nodeId` supports:

* `sql_template`
* `pass_fail_rule`
* `result_mapping`

### 2) `/run` must execute SQL checks per node scope (next big step)

Right now SQL runner uses ctx `{schemas}` only.
To be “real” you’ll want iteration modes:

* **diagram scope**: run once with `schemas`
* **table scope**: run once per table (ctx includes `schema` + `table`)
* **schema scope**: run once per schema (ctx includes `schema`)

This is the next upgrade that unlocks more powerful checks.

---

Here’s what’s left to make **`/api/v1/sql/test`** actually run cleanly in your backend (missing modules + hardening + how it plugs into the node builder + how we store mapping).

---

## 2.3 (cont.) — Required backend plumbing

# 1) Target DB connector (create a pool from `targetDatabaseUrl`)

### `apps/api/src/targetDb.ts`

```ts
import { Pool } from 'pg';

export function createTargetPool(targetDatabaseUrl: string) {
  // IMPORTANT: in production you’ll replace this with connectionId → secrets
  // v1: accept URL directly (dev only)
  return new Pool({
    connectionString: targetDatabaseUrl,
    max: 2, // keep small; these are transient pools
    idleTimeoutMillis: 5_000,
    connectionTimeoutMillis: 5_000,
  });
}
```

**Note:** In prod, don’t accept raw DB URLs from the client. Use `connectionId` + server-side secrets. For now it’s fine for internal dev.

---

# 2) Make sure `httpErrors` exists (Fastify plugin)

If you don’t already have it:

### `apps/api/src/plugins/httpErrors.ts`

```ts
import fp from 'fastify-plugin';
import fastifySensible from '@fastify/sensible';

export default fp(async (app) => {
  await app.register(fastifySensible);
});
```

Then in `apps/api/src/index.ts`:

```ts
import httpErrors from './plugins/httpErrors';
await app.register(httpErrors);
```

And install:

```bash
pnpm --filter api add @fastify/sensible fastify-plugin
```

---

Right now, `ensureSingleStatement()` blocks multiple statements, but a single `DELETE` is still possible.

Add a simple “SELECT-only” guard for `/sql/test` (builder should not mutate data).

### `apps/api/src/sqlrunner/readonly.ts`

```ts
export class SqlReadOnlyError extends Error {
  code = 'SQL_READONLY_ERROR';
}

export function ensureReadOnly(sql: string) {
  const s = sql.trim().toLowerCase();

  // allow WITH ... SELECT ...
  const isSelecty = s.startsWith('select') || s.startsWith('with');

  if (!isSelecty) throw new SqlReadOnlyError('Only SELECT/WITH queries are allowed in SQL test');

  // deny common write keywords anywhere
  const banned = [
    'insert ', 'update ', 'delete ', 'drop ', 'alter ', 'truncate ',
    'create ', 'grant ', 'revoke ', 'comment ', 'vacuum ', 'analyze ',
    'call ', 'do ', 'execute ', 'refresh ', 'set ',
  ];

  for (const kw of banned) {
    if (s.includes(kw)) throw new SqlReadOnlyError(`Disallowed keyword detected: ${kw.trim()}`);
  }

  return sql;
}
```

Patch `sqlTest.ts`:

```ts
import { ensureReadOnly } from '../sqlrunner/readonly';

const single = ensureSingleStatement(body.sql);
const ro = ensureReadOnly(single);
const finalSql = body.noAutoLimit ? ro : applyRowCap(ro, body.rowCap);
```

---

# 4) (Optional) Interpolation support in `/sql/test` (builder)

This is the most useful next improvement because it lets users test real templates:

### Patch `apps/api/src/routes/sqlTest.ts` (add)

```ts
import { interpolateSql } from '../sqlrunner/interpolate';
```

Then before `ensureSingleStatement`:

```ts
const ctx = {
  schemas: body.schemas ?? [],
  thresholds: body.thresholds ?? {},
  vars: body.vars ?? {},
};

const interpolated = interpolateSql(body.sql, ctx);
const single = ensureSingleStatement(interpolated);
const ro = ensureReadOnly(single);
const finalSql = body.noAutoLimit ? ro : applyRowCap(ro, body.rowCap);
```

Update Body schema to accept:

```ts
thresholds: z.record(z.number()).optional(),
vars: z.record(z.any()).optional(),
```

---

## 2.3 (cont.) — Builder workflow (how UI uses this)

### What frontend does when user writes SQL

1. User enters SQL
2. UI calls `POST /api/v1/sql/test`
3. UI displays:

   * `columns[]` + `rows[]`
   * `mappingSuggestions`
4. User can click “Use suggestion” which writes `result_mapping` onto the checklist node.

### What gets saved on the node

`checklist_nodes.result_mapping` = `mappingSuggestions.mapping`

Example:

```json
{
  "targetKind": "table",
  "fields": { "schema": "schema", "table": "table" }
}
```

---

## 2.3 (cont.) — What’s still missing to “ship”

You now have:

* SQL test endpoint ✅
* SQL runner engine ✅

Remaining backend pieces to fully close the loop:

### 1) Node PATCH must allow saving `result_mapping`

Make sure `PATCH /api/v1/checklist-nodes/:nodeId` supports:

* `sql_template`
* `pass_fail_rule`
* `result_mapping`

### 2) `/run` must execute SQL checks per node scope (next big step)

Right now SQL runner uses ctx `{schemas}` only.
To be “real” you’ll want iteration modes:

* **diagram scope**: run once with `schemas`
* **table scope**: run once per table (ctx includes `schema` + `table`)
* **schema scope**: run once per schema (ctx includes `schema`)

This is the next upgrade that unlocks more powerful checks.

---

## Next code bundle (pick)

If you say one of these, I’ll drop the full code:

1. **“2.3 scope iteration”**

   * table iteration (pull table list from snapshot/introspection)
   * runs sql template per table/schema
   * aggregates results into per-item outputs

2. **“2.1 nodes patch route code”**

   * full Fastify PATCH handler validating item fields
   * persists `sql_template`, `pass_fail_rule`, `result_mapping`

3. **“2.2 issues filters + sorting”**

   * implement query filters: severity/section/schema/table
   * blocking-first sort

If you don’t choose, the best next step is **(1) 2.3 scope iteration** because it turns SQL checks from “demo” into “product.”
