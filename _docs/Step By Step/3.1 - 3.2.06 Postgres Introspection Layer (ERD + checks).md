Section 3 — Postgres Introspection Layer (ERD + checks)

Below is the minimum-to-ship plan + code-ready design for 3.1 catalog snapshot and 3.2 caching/refresh that plugs directly into:

ERD graph builder (tables/columns/relationships)

SQL runner scope iteration (table lists)

Issues/annotations targeting



---

3.1 Catalog Snapshot Service

3.1.01 Data model: CatalogSnapshot

Create a stable, normalized snapshot shape.

export type CatalogSnapshot = {
  meta: {
    dbName: string;
    capturedAt: string;           // ISO
    schemas: string[];
    serverVersion?: string;
  };

  tables: Record<string, {
    key: string;                  // "schema.table"
    schema: string;
    name: string;
    kind: 'table' | 'partitioned' | 'view' | 'materialized_view';
    rowEstimate?: number;
    totalBytes?: number;
    heapBytes?: number;
    indexBytes?: number;
    toastBytes?: number;
  }>;

  columns: Record<string, {
    key: string;                  // "schema.table.column"
    tableKey: string;             // "schema.table"
    schema: string;
    table: string;
    name: string;
    dataType: string;             // formatted
    isNullable: boolean;
    default?: string | null;
  }>;

  constraints: {
    pks: Record<string, { tableKey: string; columns: string[]; name: string }>;
    uniques: Record<string, { tableKey: string; columns: string[]; name: string }>;
  };

  relationships: Record<string, {
    key: string;                  // stable signature (see below)
    name: string;
    childKey: string;             // "schema.table"
    childCols: string[];
    parentKey: string;            // "schema.table"
    parentCols: string[];
    onUpdate: string;
    onDelete: string;
    deferrable: boolean;
    initiallyDeferred: boolean;
  }>;

  indexes: Record<string, {
    key: string;                  // "schema.table.index"
    tableKey: string;
    schema: string;
    table: string;
    name: string;
    method: string;               // btree, gin, gist...
    columns: string[];            // best-effort parsed
    predicate?: string | null;
    isUnique: boolean;
    isPrimary: boolean;
  }>;

  // optional v1:
  views?: Record<string, { key: string; schema: string; name: string; definition: string }>;
};

Stable identifiers

tableKey = "${schema}.${table}"

columnKey = "${schema}.${table}.${column}"


Stable relationship signature (critical)

Use a deterministic string that survives oid changes:

relKey = `${childSchema}.${childTable}(${childCols.join(',')}) -> ${parentSchema}.${parentTable}(${parentCols.join(',')})`

(Exactly what your ERD relationship.key expects.)


---

3.1.02 Queries (Postgres system catalogs)

A) Tables + row estimates + size

row estimate: pg_class.reltuples (approx)

size: pg_total_relation_size, plus heap/index/toast via pg_relation_size


B) Columns + types + nullability + defaults

information_schema.columns or pg_attribute + pg_type

For display types, use format_type(atttypid, atttypmod)


C) PK + uniques

pg_constraint with contype in ('p','u') and conkey columns


D) Foreign keys + rules + deferrable

pg_constraint contype='f' + confupdtype, confdeltype, condeferrable, condeferred


E) Indexes + method + predicate

pg_index, pg_class, pg_am, pg_get_indexdef, indpred

Column extraction can be “best-effort” v1 (store full indexdef too if you want)


F) Views (optional v1)

pg_views or pg_get_viewdef



---

3.1.03 Implementation: snapshot pipeline (code)

apps/api/src/introspection/snapshot.ts

import type { Pool } from 'pg';
import type { CatalogSnapshot } from './types';

export async function fetchCatalogSnapshot(pool: Pool, schemas: string[]): Promise<CatalogSnapshot> {
  const capturedAt = new Date().toISOString();

  // 1) db meta
  const dbRes = await pool.query(`SELECT current_database() AS db, version() AS version`);
  const dbName = dbRes.rows?.[0]?.db ?? 'db';
  const serverVersion = dbRes.rows?.[0]?.version ?? undefined;

  // 2) tables + estimates + size
  const tablesRes = await pool.query(
    `
    SELECT
      n.nspname AS schema,
      c.relname AS name,
      c.relkind AS relkind,
      c.reltuples::bigint AS row_estimate,
      pg_total_relation_size(c.oid) AS total_bytes,
      pg_relation_size(c.oid) AS heap_bytes,
      pg_indexes_size(c.oid) AS index_bytes,
      pg_total_relation_size(c.oid) - pg_relation_size(c.oid) - pg_indexes_size(c.oid) AS toast_bytes
    FROM pg_class c
    JOIN pg_namespace n ON n.oid = c.relnamespace
    WHERE n.nspname = ANY($1::text[])
      AND c.relkind IN ('r','p','v','m')
    ORDER BY 1,2
    `,
    [schemas],
  );

  // 3) columns
  const colsRes = await pool.query(
    `
    SELECT
      n.nspname AS schema,
      c.relname AS table,
      a.attname AS name,
      format_type(a.atttypid, a.atttypmod) AS data_type,
      a.attnotnull AS not_null,
      pg_get_expr(ad.adbin, ad.adrelid) AS default_expr
    FROM pg_attribute a
    JOIN pg_class c ON c.oid = a.attrelid
    JOIN pg_namespace n ON n.oid = c.relnamespace
    LEFT JOIN pg_attrdef ad ON ad.adrelid = a.attrelid AND ad.adnum = a.attnum
    WHERE n.nspname = ANY($1::text[])
      AND c.relkind IN ('r','p','v','m')
      AND a.attnum > 0
      AND NOT a.attisdropped
    ORDER BY 1,2,a.attnum
    `,
    [schemas],
  );

  // 4) constraints (pk/unique/fk)
  const consRes = await pool.query(
    `
    SELECT
      n.nspname AS schema,
      c.relname AS table,
      con.conname AS name,
      con.contype AS type,
      con.conkey AS conkey,
      con.confkey AS confkey,
      fn.nspname AS parent_schema,
      fc.relname AS parent_table,
      con.confupdtype AS on_update,
      con.confdeltype AS on_delete,
      con.condeferrable AS deferrable,
      con.condeferred AS initially_deferred
    FROM pg_constraint con
    JOIN pg_class c ON c.oid = con.conrelid
    JOIN pg_namespace n ON n.oid = c.relnamespace
    LEFT JOIN pg_class fc ON fc.oid = con.confrelid
    LEFT JOIN pg_namespace fn ON fn.oid = fc.relnamespace
    WHERE n.nspname = ANY($1::text[])
      AND con.contype IN ('p','u','f')
    ORDER BY 1,2,3
    `,
    [schemas],
  );

  // helper: attnum array -> column names
  const attMapRes = await pool.query(
    `
    SELECT
      n.nspname AS schema,
      c.relname AS table,
      a.attnum AS attnum,
      a.attname AS col
    FROM pg_attribute a
    JOIN pg_class c ON c.oid = a.attrelid
    JOIN pg_namespace n ON n.oid = c.relnamespace
    WHERE n.nspname = ANY($1::text[])
      AND a.attnum > 0
      AND NOT a.attisdropped
    `,
    [schemas],
  );

  const attMap = new Map<string, Map<number, string>>();
  for (const r of attMapRes.rows ?? []) {
    const tkey = `${r.schema}.${r.table}`;
    if (!attMap.has(tkey)) attMap.set(tkey, new Map());
    attMap.get(tkey)!.set(Number(r.attnum), String(r.col));
  }

  // 5) indexes
  const idxRes = await pool.query(
    `
    SELECT
      n.nspname AS schema,
      t.relname AS table,
      i.relname AS name,
      am.amname AS method,
      ix.indisunique AS is_unique,
      ix.indisprimary AS is_primary,
      pg_get_indexdef(i.oid) AS indexdef,
      pg_get_expr(ix.indpred, ix.indrelid) AS predicate
    FROM pg_index ix
    JOIN pg_class t ON t.oid = ix.indrelid
    JOIN pg_class i ON i.oid = ix.indexrelid
    JOIN pg_namespace n ON n.oid = t.relnamespace
    JOIN pg_am am ON am.oid = i.relam
    WHERE n.nspname = ANY($1::text[])
    ORDER BY 1,2,3
    `,
    [schemas],
  );

  // Normalize
  const snap: CatalogSnapshot = {
    meta: { dbName, capturedAt, schemas, serverVersion },
    tables: {},
    columns: {},
    constraints: { pks: {}, uniques: {} },
    relationships: {},
    indexes: {},
  };

  // tables
  for (const r of tablesRes.rows ?? []) {
    const key = `${r.schema}.${r.name}`;
    const kind =
      r.relkind === 'r' ? 'table'
      : r.relkind === 'p' ? 'partitioned'
      : r.relkind === 'v' ? 'view'
      : 'materialized_view';

    snap.tables[key] = {
      key,
      schema: r.schema,
      name: r.name,
      kind,
      rowEstimate: Number(r.row_estimate ?? 0),
      totalBytes: Number(r.total_bytes ?? 0),
      heapBytes: Number(r.heap_bytes ?? 0),
      indexBytes: Number(r.index_bytes ?? 0),
      toastBytes: Number(r.toast_bytes ?? 0),
    };
  }

  // columns
  for (const r of colsRes.rows ?? []) {
    const tableKey = `${r.schema}.${r.table}`;
    const key = `${r.schema}.${r.table}.${r.name}`;
    snap.columns[key] = {
      key,
      tableKey,
      schema: r.schema,
      table: r.table,
      name: r.name,
      dataType: r.data_type,
      isNullable: !r.not_null,
      default: r.default_expr ?? null,
    };
  }

  // constraints → pk/unique + fks
  const decodeAction = (code: string) => {
    const m: Record<string, string> = { a: 'NO ACTION', r: 'RESTRICT', c: 'CASCADE', n: 'SET NULL', d: 'SET DEFAULT' };
    return m[code] ?? code;
  };

  for (const r of consRes.rows ?? []) {
    const tableKey = `${r.schema}.${r.table}`;
    const cols = (r.conkey ?? []).map((attnum: number) => attMap.get(tableKey)?.get(attnum)).filter(Boolean);

    if (r.type === 'p') {
      snap.constraints.pks[`${tableKey}:${r.name}`] = { tableKey, columns: cols as string[], name: r.name };
    } else if (r.type === 'u') {
      snap.constraints.uniques[`${tableKey}:${r.name}`] = { tableKey, columns: cols as string[], name: r.name };
    } else if (r.type === 'f') {
      const parentKey = `${r.parent_schema}.${r.parent_table}`;
      const parentCols = (r.confkey ?? []).map((attnum: number) => attMap.get(parentKey)?.get(attnum)).filter(Boolean);

      const relKey = `${tableKey}(${(cols as string[]).join(',')}) -> ${parentKey}(${(parentCols as string[]).join(',')})`;

      snap.relationships[relKey] = {
        key: relKey,
        name: r.name,
        childKey: tableKey,
        childCols: cols as string[],
        parentKey,
        parentCols: parentCols as string[],
        onUpdate: decodeAction(String(r.on_update ?? 'a')),
        onDelete: decodeAction(String(r.on_delete ?? 'a')),
        deferrable: !!r.deferrable,
        initiallyDeferred: !!r.initially_deferred,
      };
    }
  }

  // indexes (best-effort parse columns later; v1 store empty columns + predicate)
  for (const r of idxRes.rows ?? []) {
    const tableKey = `${r.schema}.${r.table}`;
    const key = `${r.schema}.${r.table}.${r.name}`;

    snap.indexes[key] = {
      key,
      tableKey,
      schema: r.schema,
      table: r.table,
      name: r.name,
      method: r.method,
      columns: [], // optional: parse from indexdef later
      predicate: r.predicate ?? null,
      isUnique: !!r.is_unique,
      isPrimary: !!r.is_primary,
    };
  }

  return snap;
}


---

3.2 Caching + Refresh

3.2.01 Cache strategy (minimum)

Memory cache (fast) keyed by: connectionKey + schemasHash

Optional: persist snapshot in App DB (for fast page loads)

Track:

capturedAt

expiresAt

lastRefreshedAt



Key format

cacheKey = `${connectionId || hash(targetDatabaseUrl)}::${schemas.sort().join(',')}`


---

3.2.02 App DB table (optional but recommended)

db/migrations/009_catalog_snapshots.sql

CREATE TABLE catalog_snapshots (
  id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  connection_key text NOT NULL,
  schemas text[] NOT NULL,
  captured_at timestamptz NOT NULL,
  snapshot jsonb NOT NULL,
  created_at timestamptz NOT NULL DEFAULT now()
);

CREATE INDEX catalog_snapshots_key_idx ON catalog_snapshots(connection_key);
CREATE INDEX catalog_snapshots_created_idx ON catalog_snapshots(created_at DESC);


---

3.2.03 Cache module

apps/api/src/introspection/cache.ts

import type { CatalogSnapshot } from './types';

type CacheEntry = {
  snapshot: CatalogSnapshot;
  capturedAt: number;
  expiresAt: number;
};

const mem = new Map<string, CacheEntry>();

export function getSnapshotFromCache(key: string) {
  const e = mem.get(key);
  if (!e) return null;
  if (Date.now() > e.expiresAt) {
    mem.delete(key);
    return null;
  }
  return e.snapshot;
}

export function setSnapshotCache(key: string, snapshot: CatalogSnapshot, ttlMs: number) {
  mem.set(key, {
    snapshot,
    capturedAt: Date.now(),
    expiresAt: Date.now() + ttlMs,
  });
}

export function snapshotAgeMs(key: string) {
  const e = mem.get(key);
  if (!e) return null;
  return Date.now() - e.capturedAt;
}


---

3.2.04 Snapshot endpoint (with refresh modes)

POST /api/v1/introspect/postgres/snapshot

Body

{
  "targetDatabaseUrl": "postgresql://…",
  "schemas": ["public"],
  "refresh": "cache|force|visible",
  "visibleTables": ["public.users","public.orders"]
}

Route

apps/api/src/routes/introspectSnapshot.ts

import type { FastifyInstance } from 'fastify';
import { z } from 'zod';
import { createTargetPool } from '../targetDb';
import { fetchCatalogSnapshot } from '../introspection/snapshot';
import { getSnapshotFromCache, setSnapshotCache } from '../introspection/cache';

function makeCacheKey(targetDatabaseUrl: string, schemas: string[]) {
  return `url::${targetDatabaseUrl}::schemas::${schemas.slice().sort().join(',')}`;
}

export async function introspectSnapshotRoutes(app: FastifyInstance) {
  app.post('/api/v1/introspect/postgres/snapshot', async (req) => {
    const Body = z.object({
      targetDatabaseUrl: z.string().min(10),
      schemas: z.array(z.string().min(1)).min(1),
      refresh: z.enum(['cache', 'force', 'visible']).default('cache'),
      visibleTables: z.array(z.string().min(3)).optional()
    });
    const body = Body.parse(req.body);

    const schemas = body.schemas.slice().sort();
    const key = makeCacheKey(body.targetDatabaseUrl, schemas);

    const ttlMs = 60_000; // 1 min v1

    if (body.refresh === 'cache') {
      const hit = getSnapshotFromCache(key);
      if (hit) {
        return { snapshot: hit, cache: { hit: true, key, ttlMs } };
      }
    }

    // NOTE: "visible" refresh (v1) can just run full snapshot.
    // You can optimize later by limiting columns/constraints to visibleTables.
    const pool = createTargetPool(body.targetDatabaseUrl);
    try {
      const snapshot = await fetchCatalogSnapshot(pool, schemas);
      setSnapshotCache(key, snapshot, ttlMs);
      return {
        snapshot,
        cache: { hit: false, key, ttlMs },
        lastRefreshedAt: snapshot.meta.capturedAt
      };
    } finally {
      await pool.end().catch(() => {});
    }
  });
}

Wire it in apps/api/src/index.ts:

import { introspectSnapshotRoutes } from './routes/introspectSnapshot';
await app.register(introspectSnapshotRoutes);


---

3.2.05 Manual refresh button support

Frontend just calls:

refresh=force to bypass cache


Example: POST /introspect/postgres/snapshot with refresh:"force"


---

3.2.06 Drift detection (light v1)

Add a cheap hash:

hash of: table names + column counts + relationship keys Store on snapshot meta:

meta.signature


Then compare signature to last cached to show “schema changed”.

(If you want, I’ll add that hash + a /diff endpoint next.)


---

What you can build immediately with this

ERD Graph = tables + columns + relationships

Checks scope iteration = table list

Issues filtering by schema/table uses stable keys

“Last refreshed” timestamp shows from capturedAt


If you want, I’ll do the next 3.1.XX upgrade: index column parsing (extract indexed columns from pg_get_indexdef) so checks like “FK not indexed” can be rock solid.
