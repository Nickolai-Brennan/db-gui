Next — 3.2 “Visible tables only” refresh (v1.5 optimization)

Goal: when the ERD is huge, refresh only what the user can see on the canvas:

✅ tables list still comes from schemas (or cached)

✅ but columns/constraints/indexes are fetched only for visibleTables

✅ relationships are fetched only where child/parent is in visible set (or both)

✅ same CatalogSnapshot model, but “partial” snapshots are clearly marked in meta


This is optional for v1, but it’s the key performance upgrade.


---

1) Contract (already in your snapshot endpoint)

POST /api/v1/introspect/postgres/snapshot

{
  "targetDatabaseUrl": "postgresql://…",
  "schemas": ["public"],
  "refresh": "visible",
  "visibleTables": ["public.users","public.orders"]
}

Behavior:

If refresh=visible and visibleTables[] provided:

fetch tables for those keys (and sizes/estimates)

fetch columns only for those tables

fetch constraints only for those tables (+ FK edges where either side is visible)

fetch indexes only for those tables

mark snapshot meta: { mode: "partial", visibleTablesCount }




---

2) Helper: parse visibleTables into schema/table pairs

apps/api/src/introspection/visible.ts

export type VisibleTableRef = { schema: string; table: string; key: string };

export function parseVisibleTables(keys: string[] | undefined) {
  if (!keys?.length) return [];
  const out: VisibleTableRef[] = [];
  for (const k of keys) {
    const [schema, table] = String(k).split('.');
    if (!schema || !table) continue;
    out.push({ schema, table, key: `${schema}.${table}` });
  }
  // dedupe
  const m = new Map<string, VisibleTableRef>();
  for (const t of out) m.set(t.key, t);
  return Array.from(m.values());
}


---

3) Partial snapshot fetcher

We’ll create a second fetch function that accepts an optional visible list.

apps/api/src/introspection/snapshotVisible.ts

import type { Pool } from 'pg';
import type { CatalogSnapshot } from './types';
import { computeSnapshotSignature } from './signature';
import type { VisibleTableRef } from './visible';

export async function fetchCatalogSnapshotVisible(
  pool: Pool,
  schemas: string[],
  visible: VisibleTableRef[],
): Promise<{ snapshot: CatalogSnapshot; signature: string }> {
  const capturedAt = new Date().toISOString();

  const dbRes = await pool.query(`SELECT current_database() AS db, version() AS version`);
  const dbName = dbRes.rows?.[0]?.db ?? 'db';
  const serverVersion = dbRes.rows?.[0]?.version ?? undefined;

  const visibleKeys = visible.map(v => v.key);
  const visibleSchemas = Array.from(new Set(visible.map(v => v.schema)));

  // Resolve visible table oids (fast)
  const oidRes = await pool.query(
    `
    SELECT n.nspname AS schema, c.relname AS table, c.oid
    FROM pg_class c
    JOIN pg_namespace n ON n.oid = c.relnamespace
    WHERE n.nspname = ANY($1::text[])
      AND (n.nspname || '.' || c.relname) = ANY($2::text[])
      AND c.relkind IN ('r','p','v','m')
    `,
    [schemas, visibleKeys],
  );

  const oids = (oidRes.rows ?? []).map((r: any) => Number(r.oid));
  const oidToKey = new Map<number, string>();
  for (const r of oidRes.rows ?? []) oidToKey.set(Number(r.oid), `${r.schema}.${r.table}`);

  const snap: CatalogSnapshot = {
    meta: {
      dbName,
      capturedAt,
      schemas,
      serverVersion,
      // mark partial mode for UI
      mode: 'partial' as any,
      visibleTablesCount: visibleKeys.length as any,
    } as any,
    tables: {},
    columns: {},
    constraints: { pks: {}, uniques: {} },
    relationships: {},
    indexes: {},
    views: {},
  };

  // 1) tables (only visible)
  if (oids.length) {
    const tablesRes = await pool.query(
      `
      SELECT
        n.nspname AS schema,
        c.relname AS name,
        c.relkind AS relkind,
        c.reltuples::bigint AS row_estimate,
        pg_total_relation_size(c.oid) AS total_bytes,
        pg_relation_size(c.oid) AS heap_bytes,
        pg_indexes_size(c.oid) AS index_bytes,
        pg_total_relation_size(c.oid) - pg_relation_size(c.oid) - pg_indexes_size(c.oid) AS toast_bytes
      FROM pg_class c
      JOIN pg_namespace n ON n.oid = c.relnamespace
      WHERE c.oid = ANY($1::oid[])
      ORDER BY 1,2
      `,
      [oids],
    );

    for (const r of tablesRes.rows ?? []) {
      const key = `${r.schema}.${r.name}`;
      const kind =
        r.relkind === 'r' ? 'table'
        : r.relkind === 'p' ? 'partitioned'
        : r.relkind === 'v' ? 'view'
        : 'materialized_view';

      snap.tables[key] = {
        key,
        schema: r.schema,
        name: r.name,
        kind,
        rowEstimate: Number(r.row_estimate ?? 0),
        totalBytes: Number(r.total_bytes ?? 0),
        heapBytes: Number(r.heap_bytes ?? 0),
        indexBytes: Number(r.index_bytes ?? 0),
        toastBytes: Number(r.toast_bytes ?? 0),
      };
    }
  }

  // 2) columns (only visible)
  if (oids.length) {
    const colsRes = await pool.query(
      `
      SELECT
        n.nspname AS schema,
        c.relname AS table,
        a.attname AS name,
        format_type(a.atttypid, a.atttypmod) AS data_type,
        a.attnotnull AS not_null,
        pg_get_expr(ad.adbin, ad.adrelid) AS default_expr
      FROM pg_attribute a
      JOIN pg_class c ON c.oid = a.attrelid
      JOIN pg_namespace n ON n.oid = c.relnamespace
      LEFT JOIN pg_attrdef ad ON ad.adrelid = a.attrelid AND ad.adnum = a.attnum
      WHERE c.oid = ANY($1::oid[])
        AND a.attnum > 0
        AND NOT a.attisdropped
      ORDER BY 1,2,a.attnum
      `,
      [oids],
    );

    for (const r of colsRes.rows ?? []) {
      const tableKey = `${r.schema}.${r.table}`;
      const key = `${r.schema}.${r.table}.${r.name}`;
      snap.columns[key] = {
        key,
        tableKey,
        schema: r.schema,
        table: r.table,
        name: r.name,
        dataType: r.data_type,
        isNullable: !r.not_null,
        default: r.default_expr ?? null,
      };
    }
  }

  // 3) constraints (pk/unique + FK) — include FK edges where either side is visible
  // We need attnum->name map for both visible tables AND referenced tables touched by FK.
  const consRes = await pool.query(
    `
    SELECT
      con.conname AS name,
      con.contype AS type,
      con.conrelid AS child_oid,
      con.confrelid AS parent_oid,
      con.conkey AS conkey,
      con.confkey AS confkey,
      con.confupdtype AS on_update,
      con.confdeltype AS on_delete,
      con.condeferrable AS deferrable,
      con.condeferred AS initially_deferred
    FROM pg_constraint con
    WHERE con.contype IN ('p','u','f')
      AND (
        con.conrelid = ANY($1::oid[])
        OR con.confrelid = ANY($1::oid[])
      )
    `,
    [oids],
  );

  // Build att maps for any table oids involved in these constraints
  const touchedOids = new Set<number>();
  for (const r of consRes.rows ?? []) {
    if (r.child_oid) touchedOids.add(Number(r.child_oid));
    if (r.parent_oid) touchedOids.add(Number(r.parent_oid));
  }

  const touchedArr = Array.from(touchedOids);
  const attRes = touchedArr.length
    ? await pool.query(
        `
        SELECT
          a.attrelid AS oid,
          a.attnum AS attnum,
          a.attname AS col
        FROM pg_attribute a
        WHERE a.attrelid = ANY($1::oid[])
          AND a.attnum > 0
          AND NOT a.attisdropped
        `,
        [touchedArr],
      )
    : { rows: [] as any[] };

  const oidAttMap = new Map<number, Map<number, string>>();
  for (const r of attRes.rows ?? []) {
    const oid = Number(r.oid);
    if (!oidAttMap.has(oid)) oidAttMap.set(oid, new Map());
    oidAttMap.get(oid)!.set(Number(r.attnum), String(r.col));
  }

  // helper: oid -> schema.table
  const oidKeyRes = touchedArr.length
    ? await pool.query(
        `
        SELECT n.nspname AS schema, c.relname AS table, c.oid
        FROM pg_class c
        JOIN pg_namespace n ON n.oid = c.relnamespace
        WHERE c.oid = ANY($1::oid[])
        `,
        [touchedArr],
      )
    : { rows: [] as any[] };

  for (const r of oidKeyRes.rows ?? []) {
    oidToKey.set(Number(r.oid), `${r.schema}.${r.table}`);
  }

  const decodeAction = (code: string) => {
    const m: Record<string, string> = { a: 'NO ACTION', r: 'RESTRICT', c: 'CASCADE', n: 'SET NULL', d: 'SET DEFAULT' };
    return m[code] ?? code;
  };

  for (const r of consRes.rows ?? []) {
    const type = String(r.type);
    const childOid = Number(r.child_oid);
    const parentOid = Number(r.parent_oid);

    const childKey = oidToKey.get(childOid);
    if (!childKey) continue;

    const childCols = (r.conkey ?? []).map((n: number) => oidAttMap.get(childOid)?.get(Number(n))).filter(Boolean) as string[];

    if (type === 'p') {
      snap.constraints.pks[`${childKey}:${r.name}`] = { tableKey: childKey, columns: childCols, name: r.name };
    } else if (type === 'u') {
      snap.constraints.uniques[`${childKey}:${r.name}`] = { tableKey: childKey, columns: childCols, name: r.name };
    } else if (type === 'f') {
      const parentKey = oidToKey.get(parentOid) ?? `oid:${parentOid}`;
      const parentCols = (r.confkey ?? []).map((n: number) => oidAttMap.get(parentOid)?.get(Number(n))).filter(Boolean) as string[];

      // Only keep relationship if either side is visible
      const childVisible = visibleKeys.includes(childKey);
      const parentVisible = visibleKeys.includes(parentKey);
      if (!childVisible && !parentVisible) continue;

      const relKey = `${childKey}(${childCols.join(',')}) -> ${parentKey}(${parentCols.join(',')})`;

      snap.relationships[relKey] = {
        key: relKey,
        name: r.name,
        childKey,
        childCols,
        parentKey,
        parentCols,
        onUpdate: decodeAction(String(r.on_update ?? 'a')),
        onDelete: decodeAction(String(r.on_delete ?? 'a')),
        deferrable: !!r.deferrable,
        initiallyDeferred: !!r.initially_deferred,
      };
    }
  }

  // 4) indexes (only visible tables) — reuse your improved index query but restrict by oids
  if (oids.length) {
    const idxRes = await pool.query(
      `
      SELECT
        n.nspname AS schema,
        t.relname AS table,
        i.relname AS name,
        am.amname AS method,
        ix.indisunique AS is_unique,
        ix.indisprimary AS is_primary,
        ix.indkey AS indkey,
        ix.indnkeyatts AS indnkeyatts,
        pg_get_indexdef(i.oid) AS indexdef,
        pg_get_expr(ix.indpred, ix.indrelid) AS predicate,
        array_remove(array_agg(a.attname ORDER BY x.ord), NULL) AS attnames,
        array_remove(array_agg(pg_get_indexdef(i.oid, x.ord, true) ORDER BY x.ord), NULL) AS exprs
      FROM pg_index ix
      JOIN pg_class t ON t.oid = ix.indrelid
      JOIN pg_class i ON i.oid = ix.indexrelid
      JOIN pg_namespace n ON n.oid = t.relnamespace
      JOIN pg_am am ON am.oid = i.relam
      JOIN LATERAL generate_subscripts(ix.indkey, 1) AS x(ord) ON true
      LEFT JOIN pg_attribute a
        ON a.attrelid = t.oid
       AND a.attnum = ix.indkey[x.ord]
       AND ix.indkey[x.ord] <> 0
      WHERE t.oid = ANY($1::oid[])
      GROUP BY
        n.nspname, t.relname, i.relname, am.amname,
        ix.indisunique, ix.indisprimary, ix.indkey, ix.indnkeyatts,
        i.oid, ix.indpred, ix.indrelid
      ORDER BY 1,2,3
      `,
      [oids],
    );

    for (const r of idxRes.rows ?? []) {
      const tableKey = `${r.schema}.${r.table}`;
      const key = `${r.schema}.${r.table}.${r.name}`;

      const attnames: string[] = (r.attnames ?? []).map(String);
      const exprs: string[] = (r.exprs ?? []).map(String);

      const parts: Array<{ kind: 'col' | 'expr'; value: string }> = [];
      const indnkeyatts = Number(r.indnkeyatts ?? attnames.length);

      const maxOrd = Math.max(attnames.length, exprs.length);
      for (let i = 0; i < maxOrd; i++) {
        const col = attnames[i];
        if (col) parts.push({ kind: 'col', value: col });
        else if (exprs[i]) parts.push({ kind: 'expr', value: exprs[i] });
      }

      const keyParts = parts.slice(0, indnkeyatts);
      const includeParts = parts.slice(indnkeyatts);

      const keyColumns = keyParts.filter(p => p.kind === 'col').map(p => p.value);
      const expressions = keyParts.filter(p => p.kind === 'expr').map(p => p.value);
      const includeColumns = includeParts.filter(p => p.kind === 'col').map(p => p.value);

      snap.indexes[key] = {
        key,
        tableKey,
        schema: r.schema,
        table: r.table,
        name: r.name,
        method: r.method,
        keyColumns,
        includeColumns,
        expressions,
        predicate: r.predicate ?? null,
        isUnique: !!r.is_unique,
        isPrimary: !!r.is_primary,
        indexDef: r.indexdef ?? null,
      } as any;
    }
  }

  // Views definitions for visible views
  // If a visible table is a view/matview, include its definition
  if (oids.length) {
    const viewDefRes = await pool.query(
      `
      SELECT
        n.nspname AS schema,
        c.relname AS name,
        c.relkind AS relkind,
        pg_get_viewdef(c.oid, true) AS definition
      FROM pg_class c
      JOIN pg_namespace n ON n.oid = c.relnamespace
      WHERE c.oid = ANY($1::oid[])
        AND c.relkind IN ('v','m')
      `,
      [oids],
    );

    for (const r of viewDefRes.rows ?? []) {
      const key = `${r.schema}.${r.name}`;
      const kind = r.relkind === 'v' ? 'view' : 'materialized_view';
      snap.views![key] = {
        key, schema: r.schema, name: r.name, kind,
        definition: String(r.definition ?? ''),
      };
    }
  }

  const signature = computeSnapshotSignature(snap);
  return { snapshot: snap, signature };
}


---

4) Wire “visible” mode into the snapshot endpoint

Patch apps/api/src/routes/introspectSnapshot.ts:

1. import:



import { parseVisibleTables } from '../introspection/visible';
import { fetchCatalogSnapshotVisible } from '../introspection/snapshotVisible';

2. in the “fresh introspection” branch:



if (body.refresh === 'visible') {
  const visible = parseVisibleTables(body.visibleTables);
  const { snapshot, signature } = await fetchCatalogSnapshotVisible(pool, schemas, visible);

  // compare signature / persist / mem cache (same as before)
  ...
  return { snapshot, ... };
}

> For caching: you can still store partial snapshots, but I recommend:



don’t overwrite your full snapshot in DB

only keep partial in memory So the ERD can always “refresh full” when needed.


If you want that behavior, use a different cacheKey suffix: cacheKey += "::mode:partial"


---

5) What frontend should do

On canvas pan/zoom:

compute visible table keys

debounce (500–1000ms)

call snapshot endpoint with refresh=visible and those keys

merge partial snapshot into current snapshot in UI store (only replace those tables’ columns/indexes/constraints)




---

Next (recommended)

With visible refresh in place, the next most valuable addition is “relationship-aware anchors” for the ERD UI:

when an issue targets a relationship key, the ERD highlights the edge and scrolls to it. (Backend already supports it via target_ref + focus key.)


If you want the backend helper for this, say: “ERD highlight payload”.
